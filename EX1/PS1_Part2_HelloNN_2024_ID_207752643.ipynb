{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LuoXusr9B3nd"
      },
      "source": [
        "# PS1: Your first library-free neural network!  \n",
        "\n",
        "Advanced Learning 2024\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fRYKvBtcZj44"
      },
      "source": [
        "For SUBMISSION:   \n",
        "\n",
        "Please upload the complete and executed `ipynb` to your git repository. Verify that all of your output can be viewed directly from github, and provide a link to that git file below.\n",
        "\n",
        "~~~\n",
        "STUDENT ID: 207752643\n",
        "~~~\n",
        "\n",
        "~~~\n",
        "STUDENT GIT LINK: https://github.com/lioravraham\n",
        "~~~\n",
        "In Addition, don't forget to add your ID to the files:    \n",
        "  \n",
        "`PS1_Part2_HelloNN_2024_ID_[207752643].html`   \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X2ZxCWBO_IIT"
      },
      "outputs": [],
      "source": [
        "import numpy as np # You are allowed to use  only numpy.\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
        "import torch\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f66laqMmMouB",
        "outputId": "64962b0a-a39b-41ab-c0e5-db040db30497"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Num GPUs Available:  0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CnAz84ZL_9XQ"
      },
      "source": [
        "\n",
        "**Welcome**.   \n",
        "\n",
        "In this part of the problem set you are set to build a complete and flexible neural network.  \n",
        "This neural network will be library free (in the sense that we won't use PyTorch/Tensorflow/etc.).   \n",
        "\n",
        "Let's do a quick review of the basic neural-network components:  \n",
        "\n",
        "\n",
        "*   *Layer* - can be fully connected (dense/hidden), convolution, etc.\n",
        "  * Forward propagation- the layer outputs the next layer's input\n",
        "  * Backward propagation- the layer also outputs the gradient descent update\n",
        "*   *Activation* Layer (e.g. ReLU) - there are no parameters, only gradients with respect to the input. We want to compute both the gradient w.r.t the parameters of the layer and to create the gradient with respect to the layer's inputs\n",
        "   * *Forward propagation*- the layer outputs the next layer's input\n",
        "   * *Backward propagation*- the layer also outputs the gradient descent update\n",
        "*   *Loss Function* : how our model  quantifies the difference between the predicted outputs the actual (target) values  \n",
        "*   *Network Wrapper*-  wraps our components together as a trainable model.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ncQsamml85JG"
      },
      "source": [
        "Useful resource:  \n",
        "* Gradient descent for neural networks [cheat sheet](https://moodle4.cs.huji.ac.il/hu23/mod/resource/view.php?id=402297).\n",
        "* Neural network architecture [cheat sheet](https://moodle4.cs.huji.ac.il/hu23/mod/url/view.php?id=402298)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P11k0GECXiR-"
      },
      "source": [
        "### 0. Loading data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lwLDOo7IXfcI"
      },
      "source": [
        "You are going to test and evaluate your home-made network on the `mnist` dataset.   \n",
        "The MNIST dataset is a large dataset of handwritten digits that is commonly used for training various image and vision models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RIxpddzDXgBN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e288be9b-a749-40ab-c401-f45031e341b5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 0us/step\n"
          ]
        }
      ],
      "source": [
        "from keras.datasets import mnist\n",
        "from keras.utils import to_categorical\n",
        "# load MNIST from server\n",
        "# Using a standard library (keras.datasets) to load the mnist data\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B_yaFDAtXj1h"
      },
      "source": [
        "#### Data transformations\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3PooYSGAgY4v"
      },
      "outputs": [],
      "source": [
        "# training data : 60000 samples\n",
        "# reshape and normalize input data\n",
        "x_train = x_train.reshape(x_train.shape[0], 1, 28*28)\n",
        "x_train = x_train.astype('float32')\n",
        "x_train /= 255\n",
        "# One-hot encoding of the output.\n",
        "# Currently a number in range [0,9]; Change into a vector of size 10\n",
        "# e.g. number 3 will become [0, 0, 0, 1, 0, 0, 0, 0, 0, 0]\n",
        "y_train = to_categorical(y_train)\n",
        "# same for test data : 10000 samples\n",
        "x_test = x_test.reshape(x_test.shape[0], 1, 28*28)\n",
        "x_test = x_test.astype('float32')\n",
        "x_test /= 255\n",
        "y_test = to_categorical(y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q8HGS8h1uXAD"
      },
      "source": [
        "### 1. Network's Components"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hda7HDt6NMg1"
      },
      "source": [
        "Please fill-in the missing code in the code boxes below (only where  `#### SOLUTION REQUIRED ####` is specified).   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zi3-57RaG-YW"
      },
      "outputs": [],
      "source": [
        "\n",
        "# This class is a general layer primitive, defining that each instance must\n",
        "# have an (input,output) parameters, and 2 functions: forward+backward propogation\n",
        "class Layer_Primitive:\n",
        "    def __init__(self):\n",
        "        self.input = None\n",
        "        self.output = None\n",
        "\n",
        "\n",
        "    # computes the output Y of a layer for a given input X\n",
        "    def forward_propagation(self, input):\n",
        "      raise NotImplementedError\n",
        "\n",
        "    # computes dE/dX for a given dE/dY (and update parameters if any)\n",
        "    def helper_parameter_backward_propagation(self, output_error, learning_rate):\n",
        "        raise NotImplementedError\n",
        "\n",
        "\n",
        "\n",
        "    # computes dE/dX for a given dE/dY (and update parameters if any)\n",
        "    def backward_propagation(self, output_error, learning_rate):\n",
        "        raise NotImplementedError"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kFrfWvZFoxGz"
      },
      "source": [
        "#### Fully Connected Layer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fZnNmnfjDqBg"
      },
      "source": [
        "A fully-connected layer (a.k.a. affine, dense,linear layer) connects every input neuron to every output neuron.   \n",
        "It has 2 parameters: (input, output).   \n",
        "You need to define (code) the following:\n",
        "* its initialization weights with random weights.\n",
        "* the forward propogation calculation (as shown in class).\n",
        "* the backward propogation gradients calculation (given output, as shown in class).\n",
        "\n",
        "Parameters must be intitialized with some values. There are many ways to initialize the weights, and you are encouraged to do a quick research about the common methods. Any commonly used method will be accepted.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b2vegoAGNSdm"
      },
      "source": [
        "1.1 (20 pts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1oPtObGHL0qA"
      },
      "outputs": [],
      "source": [
        "#### SOLUTION REQUIRED ####\n",
        "# inherit from base class Layer\n",
        "class Affine_Layer(Layer_Primitive):\n",
        "    # input_size = number of input neurons\n",
        "    # output_size = number of output neurons\n",
        "    def __init__(self, input_size, output_size):\n",
        "        self.weights = np.random.randn(input_size, output_size) * 0.01\n",
        "        self.bias =  np.random.randn(1, output_size)\n",
        "\n",
        "    def sigmoid(self, x):\n",
        "        return 1 / (1 + np.exp(-x))\n",
        "\n",
        "    # returns output for a given input\n",
        "    def forward_propagation(self, input_data):\n",
        "        self.input = input_data\n",
        "        self.output = np.dot(self.input, self.weights) + self.bias  # Linear combination\n",
        "        return self.output\n",
        "\n",
        "    # computes dE/dW, dE/dB for a given output_error=dE/dY. Returns input_error=dE/dX.\n",
        "    def backward_propagation(self, output_grad, learning_rate):\n",
        "        input_error = np.dot(output_grad, self.weights.T) #dE/dX = dE/dY * dY/dX\n",
        "        bias_error = output_grad.sum(axis=0, keepdims=True)  # dE/dB = sum(dE/dY)\n",
        "        weights_error = self.input.T @ output_grad  # dE/dW = X^T * dE/dY\n",
        "\n",
        "        # update parameters\n",
        "        self.weights -= learning_rate * weights_error\n",
        "        self.bias -= learning_rate * bias_error\n",
        "        return input_error\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uktf9H2UuhYR"
      },
      "source": [
        "#### Activation layers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nbjwalPGEgLy"
      },
      "source": [
        "Activation functions are often a non-linear functions that aid in how well the network model adapts to and learns  the training dataset. The choice of activation function in the output layer will define the type of predictions the model can make.  \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mg5f_-ikVMzi"
      },
      "outputs": [],
      "source": [
        "# inherit from base class Layer\n",
        "class ActivationLayer(Layer_Primitive):\n",
        "    def __init__(self, activation, activation_grad):\n",
        "        self.activation = activation\n",
        "        self.activation_grad = activation_grad\n",
        "\n",
        "    # returns the activated input\n",
        "    def forward_propagation(self, input_data):\n",
        "        self.input = input_data\n",
        "        self.output = self.activation(self.input)\n",
        "        return self.output\n",
        "\n",
        "    # Returns input_error=dE/dX for a given output_grad=dE/dY.\n",
        "    # learning_rate is not used because there is no \"learnable\" parameters.\n",
        "    def backward_propagation(self, output_grad, learning_rate):\n",
        "        return self.activation_grad(self.input) * output_grad\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FaB7AX-aFP1j"
      },
      "source": [
        "\n",
        "You need to define (code) the following via different functions:\n",
        "* the forward propogation calculation (as shown in class).\n",
        "* the backward propogation gradients calculation (given output, as shown in class)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fbvilWixNaro"
      },
      "source": [
        "1.2 (20 pts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4-M9-LPBgBTB"
      },
      "outputs": [],
      "source": [
        "#### SOLUTION REQUIRED ####\n",
        "\n",
        "# activation functions and their derivatives:\n",
        "\n",
        "def tanh(x):\n",
        "    return np.tanh(x)\n",
        "\n",
        "def tanh_grad(x):\n",
        "    return 1 - np.tanh(x)**2\n",
        "\n",
        "def relu(x):\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "def relu_grad(x):\n",
        "    return np.where(x > 0, 1, 0)\n",
        "\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def sigmoid_grad(x):\n",
        "    s = sigmoid(x)\n",
        "    return s * (1 - s)\n",
        "\n",
        "def softmax(x):\n",
        "    e_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
        "    return e_x / np.sum(e_x, axis=1, keepdims=True)\n",
        "\n",
        "def grad_softmax(x):\n",
        "  sm = softmax(x)\n",
        "  return sm * (1 - sm)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s3RBktf7uowi"
      },
      "source": [
        "#### Loss function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RgZ4SRFkG_Sj"
      },
      "source": [
        "1.3 (10 pts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uo_FcJrYgQaB"
      },
      "outputs": [],
      "source": [
        "#### SOLUTION REQUIRED ####\n",
        "\n",
        "# loss function and its derivative\n",
        "\n",
        "def mse(y_true, y_pred):\n",
        "    return np.mean((y_true - y_pred)**2)\n",
        "\n",
        "def mse_grad(y_true, y_pred):\n",
        "    return 2 * (y_pred - y_true) / y_true.shape[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V4jPsOT9uy-_"
      },
      "source": [
        "#### Putting everything together"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fEK-tOfrNhO_"
      },
      "source": [
        "1.4 (10 pts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fMwSnK5pgV9Y"
      },
      "outputs": [],
      "source": [
        "#### SOLUTION REQUIRED (in `predict`) ####\n",
        "\n",
        "class MyNetwork:\n",
        "    def __init__(self):\n",
        "        self.layers = []\n",
        "        self.loss = None\n",
        "        self.loss_grad = None\n",
        "\n",
        "    # add layer to network\n",
        "    def add(self, layer):\n",
        "        self.layers.append(layer)\n",
        "\n",
        "    # set loss to use\n",
        "    def use_loss(self, loss, loss_grad):\n",
        "        self.loss = loss\n",
        "        self.loss_grad = loss_grad\n",
        "\n",
        "\n",
        "    # train the network\n",
        "    def fit(self, x_train, y_train, epochs, learning_rate):\n",
        "        # sample dimension first\n",
        "        samples = len(x_train)\n",
        "\n",
        "        # training loop\n",
        "        for i in range(epochs):\n",
        "            err = 0\n",
        "            for j in range(samples):\n",
        "                # forward propagation\n",
        "                output = x_train[j]\n",
        "                for layer in self.layers:\n",
        "                    output = layer.forward_propagation(output)\n",
        "\n",
        "                # compute loss (for display purpose only)\n",
        "                err += self.loss(y_train[j], output)\n",
        "\n",
        "                # backward propagation\n",
        "                grad = self.loss_grad(y_train[j], output)\n",
        "                for layer in reversed(self.layers):\n",
        "                    grad = layer.backward_propagation(grad, learning_rate)\n",
        "\n",
        "            # calculate average error on all samples\n",
        "            err /= samples\n",
        "            print('Training epoch %d/%d   error=%f' % (i+1, epochs, err))\n",
        "\n",
        "\n",
        "    # predict output for given input\n",
        "    def predict(self, x_test,y_test=np.array([])):\n",
        "        if y_test.size:\n",
        "           assert len(x_test)==len(y_test) # if Y is given\n",
        "        # sample dimension first\n",
        "        samples = len(x_test)\n",
        "        result = []\n",
        "        loss = 0\n",
        "        correct = 0\n",
        "        # run network over all samples\n",
        "        for i in range(samples):\n",
        "            # forward propagation\n",
        "            output = x_test[i]\n",
        "            for layer in self.layers:\n",
        "                output = layer.forward_propagation(output)\n",
        "            result.append(output)\n",
        "            # ONLY IF LABELS ARE GIVEN (Y):\n",
        "            if y_test.size:\n",
        "                # Evaluate the output against Y,\n",
        "                # calculate loss against Y, add to `loss`:\n",
        "                loss += self.loss(y_test[i], output)\n",
        "                target = y_test[i]\n",
        "                # Evaluate the label of the output against real, and if identical,\n",
        "                # add +1 to `correct`:\n",
        "                if np.argmax(output) == np.argmax(target): # if the model's predicted class is the same as the true class\n",
        "                   correct += 1\n",
        "        if y_test.size:\n",
        "            mean_loss = loss/samples\n",
        "\n",
        "            print('\\nTest set: Avg. loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.\n",
        "                  format(mean_loss, correct, samples,100. * correct / samples))\n",
        "\n",
        "        return result\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dCXXcALGXSRb"
      },
      "source": [
        "## 2. Testing Your Neural Network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KotuuqWKXt2r"
      },
      "source": [
        "### Defining our main neural network architecture"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wzatylPzQIdR"
      },
      "source": [
        "Define your network's architecture:  \n",
        "(Please rationalize your choice of activation funciton.)\n",
        "* first affine layer that takes your input and outputs 128 nodes\n",
        "* `tanh/relu/sigmoid` activation layer following the first affine layer\n",
        "* second affine layer that takes the first layer's input and outputs 64 nodes\n",
        "* `tanh/relu/sigmoid` activation layer following the second affine layer\n",
        "* third affine layer that takes your second layer's input and outputs nodes in the size of the Y labels.\n",
        "* `tanh/relu/sigmoid` activation layer following the last affine layer\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kSwlLJXWNqii"
      },
      "source": [
        "2.1 (5 pts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_O9Mi5Qmuvlp"
      },
      "outputs": [],
      "source": [
        "#### SOLUTION REQUIRED (in `predict`) ####\n",
        "\n",
        "# Network Architecture\n",
        "net = MyNetwork()\n",
        "net.add(Affine_Layer(input_size=28*28, output_size=128))\n",
        "net.add(ActivationLayer(relu, relu_grad))\n",
        "net.add(Affine_Layer(input_size=128, output_size=64))\n",
        "net.add(ActivationLayer(relu, relu_grad))\n",
        "net.add(Affine_Layer(input_size=64, output_size=10)) #the MNIST dataset contains 10 classes, representing digits from 0 to 9\n",
        "net.add(ActivationLayer(sigmoid, sigmoid_grad))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "for the first and the second layer i choose Relu function.\n",
        "ReLU activation works well in hidden layers because it introduces non-linearity without vanishing gradient issues.\n",
        "for the last layer i choose sigmoid activation function.\n",
        "Sigmoid is ideal for binary classification since it outputs probabilities in the range [0, 1]."
      ],
      "metadata": {
        "id": "iTrjvVqR_A5U"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o8_5gnOuuxWC"
      },
      "source": [
        "### Training!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QhWuBFBfg3SB",
        "outputId": "8a7b8131-c585-45e4-812f-c3b8a0c72f57"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training epoch 1/20   error=0.090326\n",
            "Training epoch 2/20   error=0.087719\n",
            "Training epoch 3/20   error=0.060202\n",
            "Training epoch 4/20   error=0.034390\n",
            "Training epoch 5/20   error=0.022105\n",
            "Training epoch 6/20   error=0.017201\n",
            "Training epoch 7/20   error=0.014412\n",
            "Training epoch 8/20   error=0.012475\n",
            "Training epoch 9/20   error=0.011027\n",
            "Training epoch 10/20   error=0.009904\n",
            "Training epoch 11/20   error=0.009005\n",
            "Training epoch 12/20   error=0.008256\n",
            "Training epoch 13/20   error=0.007630\n",
            "Training epoch 14/20   error=0.007098\n",
            "Training epoch 15/20   error=0.006639\n",
            "Training epoch 16/20   error=0.006242\n",
            "Training epoch 17/20   error=0.005891\n",
            "Training epoch 18/20   error=0.005578\n",
            "Training epoch 19/20   error=0.005298\n",
            "Training epoch 20/20   error=0.005043\n",
            "Total process time: 1508.521\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# While developing, it is recommended to train your model on a subset of the data... / or low epochs.\n",
        "# as we didn't implemented mini-batch GD, training will be pretty slow if we update at each iteration on 60000 samples...\n",
        "net.use_loss(mse, mse_grad)\n",
        "epoch_num = 20\n",
        "lr = 0.01\n",
        "t1 = time.time()\n",
        "net.fit(x_train, y_train, epochs=epoch_num, learning_rate=lr)\n",
        "\n",
        "print(f\"Total process time: {round(time.time() - t1,3)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kXwnmpjlu5sa"
      },
      "source": [
        "### Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hhz13JznTujh"
      },
      "source": [
        "Exciting! Now is the time to test your model.   \n",
        "\n",
        "    May the gradients be always in your favor."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BHDYRUoq54Fk",
        "outputId": "efc378e7-e517-4d34-d056-bddd32c81bd0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Avg. loss: 0.0065, Accuracy: 9617/10000 (96%)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "output = net.predict(x_test ,y_test )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S9HEZ6ElvVVj"
      },
      "source": [
        "## 3. Benchmarking against PyTorch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z-0UWnaYUNz7"
      },
      "source": [
        "How well your model performs against a similar-architecture PyTorch model?   \n",
        "It is time to find out:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R2TeiObsnBr1"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import TensorDataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-h8cCoV3ZSkt"
      },
      "source": [
        "#### Prepare the data as tensors using PyTorch DataLoader:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1rqwlzUIvFCZ"
      },
      "outputs": [],
      "source": [
        "t_train =  TensorDataset(torch.Tensor(x_train),torch.Tensor(y_train))\n",
        "t_test =  TensorDataset(torch.Tensor(x_test),torch.Tensor(y_test))\n",
        "train_loader = torch.utils.data.DataLoader(dataset=t_train, batch_size=64, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(dataset=t_test, batch_size=64, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Ngm-Gv_UsCV"
      },
      "source": [
        "Define a `PyTorchNet` class with an identical architecture you used in your home-made network."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mJBs2JsyNxid"
      },
      "source": [
        "3.1 (10 pts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Ed2P1LmUpgS"
      },
      "outputs": [],
      "source": [
        "#### SOLUTION REQUIRED  ####\n",
        "\n",
        "class PyTorchNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(PyTorchNet, self).__init__()\n",
        "        input_size = 28 * 28 #input of 28 * 28 pixel\n",
        "        num_classes = 10 #number of output lasses\n",
        "\n",
        "        self.fc1 = nn.Linear(input_size, 128)\n",
        "        self.activ1 = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(128, 64)\n",
        "        self.activ2 = nn.ReLU()\n",
        "        self.fc3 = nn.Linear(64, num_classes)\n",
        "        self.activ3 = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 28 * 28)\n",
        "        x = self.fc1(x)\n",
        "        x = self.activ1(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.activ2(x)\n",
        "        x = self.fc3(x)\n",
        "        #x = self.activ3(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fG-8BEdDlL4L",
        "outputId": "9c882881-d147-4f95-c5ae-f419891396ca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/20], Step [500/938], Loss: 0.0118\n",
            "Epoch [2/20], Step [500/938], Loss: 0.0140\n",
            "Epoch [3/20], Step [500/938], Loss: 0.0144\n",
            "Epoch [4/20], Step [500/938], Loss: 0.0027\n",
            "Epoch [5/20], Step [500/938], Loss: 0.0115\n",
            "Epoch [6/20], Step [500/938], Loss: 0.0063\n",
            "Epoch [7/20], Step [500/938], Loss: 0.0104\n",
            "Epoch [8/20], Step [500/938], Loss: 0.0052\n",
            "Epoch [9/20], Step [500/938], Loss: 0.0039\n",
            "Epoch [10/20], Step [500/938], Loss: 0.0095\n",
            "Epoch [11/20], Step [500/938], Loss: 0.0117\n",
            "Epoch [12/20], Step [500/938], Loss: 0.0108\n",
            "Epoch [13/20], Step [500/938], Loss: 0.0081\n",
            "Epoch [14/20], Step [500/938], Loss: 0.0041\n",
            "Epoch [15/20], Step [500/938], Loss: 0.0030\n",
            "Epoch [16/20], Step [500/938], Loss: 0.0133\n",
            "Epoch [17/20], Step [500/938], Loss: 0.0070\n",
            "Epoch [18/20], Step [500/938], Loss: 0.0112\n",
            "Epoch [19/20], Step [500/938], Loss: 0.0047\n",
            "Epoch [20/20], Step [500/938], Loss: 0.0041\n",
            "Total process time: 101.553\n"
          ]
        }
      ],
      "source": [
        "# Train the model\n",
        "num_epochs = 20\n",
        "pt_learning_rate = 0.01\n",
        "pt_network = PyTorchNet()\n",
        "optimizer = torch.optim.Adam(pt_network.parameters(), lr=pt_learning_rate)\n",
        "criterion = nn.MSELoss() #? להמיר לOnehot vetor את הלייבלס ולהשתמש בMSE\n",
        "#we can use softmax loss or cross entropy loss\n",
        "t1 = time.time()\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "        # Forward pass\n",
        "        outputs = pt_network(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        # Backward pass and optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        # A handy printout:\n",
        "        if (i + 1) % 500 == 0:\n",
        "            print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(train_loader)}], Loss: {loss.item():.4f}')\n",
        "\n",
        "print(f\"Total process time: {round(time.time() - t1,3)}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L3rFfBfaV3Gt"
      },
      "source": [
        "Evaluation:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RsfDSk2IrXst",
        "outputId": "8ca741d0-6e39-4cd7-f15a-8045c9e2c9b6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Avg. loss: 0.0001, Accuracy: 9526/10000 (95%)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "pt_network.eval()\n",
        "test_losses = []\n",
        "test_loss = 0\n",
        "correct = 0\n",
        "with torch.no_grad():\n",
        "    for data, target in test_loader:\n",
        "        output = pt_network(data)\n",
        "        test_loss += criterion(output, target,)\n",
        "        pred = output.data.max(1, keepdim=True)[1]\n",
        "        correct += pred.eq(target.data.max(1,keepdim=True)[1]).sum()\n",
        "\n",
        "test_loss /= len(test_loader.dataset)\n",
        "test_losses.append(test_loss)\n",
        "print('\\nTest set: Avg. loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "  test_loss, correct, len(test_loader.dataset),\n",
        "  100. * correct / len(test_loader.dataset)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EyKXHGW3XsAN"
      },
      "source": [
        "3.2 (10 pts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dI13KBVgrBkc"
      },
      "source": [
        "Time for some questions:\n",
        "1. Which one of the models performed better? Why?\n",
        "2. Which one of the models performed faster? Why?  \n",
        "3. What would you change in your network's architecture?   \n",
        "4. What would you change in your model's solution algorithm?"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "A6_A8hPicH74"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cb6AMtbuXvSR"
      },
      "source": [
        "Write your solutions here:\n",
        "1. The second model (MyNetwork) achieved a slightly higher accuracy of 96.17%, compared to the first model (PyTorch) with 95.26%. Several factors could explain this performance difference, with the most significant being that the manual model used online learning, updating weights after each sample, while the PyTorch model used batch training. Also, it is important to note that the difference is not very significant (about ~ 1%).\n",
        "\n",
        "\n",
        "2. The first model (PyTorch) performed faster (Total process time: ~144.671 against ~7674.063 for Mynetwork). This is primarily because it uses batch training, which allows for more efficient computation compared to the online learning method used by the second model (MyNetwork), where the weights are updated after each individual sample. Batch training in PyTorch allows for parallel processing of multiple samples at once, significantly speeding up the training process.\n",
        "\n",
        "\n",
        "3. To improve the network's architecture, I would make the following changes:\n",
        "- Replace the sigmoid activation function in the final layer with softmax, which is more suitable for multi-class classification tasks like MNIST.\n",
        "\n",
        "- Use Cross-Entropy Loss instead of MSE, as it is specifically designed for classification problems and handles the output probabilities better.\n",
        "\n",
        "4. To improve the model's solution algorithm, I would make the following changes:\n",
        "- Batch Training: Switch from online learning (updating weights after each sample) to batch training or mini-batch gradient descent to improve training stability and speed.\n",
        "- Early Stopping: Introduce early stopping to halt training when the model's performance on the validation set stops improving, preventing overfitting."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mmJYjygWrCAg"
      },
      "source": [
        "## 4. The Network Wars!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y_bdbde5Wj-y"
      },
      "source": [
        "Here is your chance to play with your model's architecture in order to break your own benchmark set eariler.  \n",
        "You can add/remove layers, play with their sizes, types, etc.   \n",
        "You can add a new loss if you wish, or anything else that will fairly give your model an advantage over base.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oG2hhaeyN59O"
      },
      "source": [
        "4.1 (15 pts)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def cross_entropy_loss(y_true, y_pred):\n",
        "  sy_pred = softmax(y_pred)\n",
        "  return -np.sum(y_true * np.log(sy_pred + 1e-12))\n",
        "\n",
        "def cross_entropy_grad(y_true, y_pred):\n",
        "  return softmax(y_pred) - y_true"
      ],
      "metadata": {
        "id": "rIw4LkCQNCVh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "improve_net = MyNetwork()\n",
        "improve_net.add(Affine_Layer(input_size=28*28, output_size=128))\n",
        "improve_net.add(ActivationLayer(relu, relu_grad))\n",
        "improve_net.add(Affine_Layer(input_size=128, output_size=64))\n",
        "improve_net.add(ActivationLayer(relu, relu_grad))\n",
        "improve_net.add(Affine_Layer(input_size=64, output_size=10)) #the MNIST dataset contains 10 classes, representing digits from 0 to 9\n",
        "#improve_net.add(ActivationLayer(softmax, grad_softmax))"
      ],
      "metadata": {
        "id": "pjS2M4Q4R_j3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# While developing, it is recommended to train your model on a subset of the data... / or low epochs.\n",
        "# as we didn't implemented mini-batch GD, training will be pretty slow if we update at each iteration on 60000 samples...\n",
        "improve_net.use_loss(cross_entropy_loss, cross_entropy_grad)\n",
        "epoch_num = 20\n",
        "lr = 0.01\n",
        "t1 = time.time()\n",
        "improve_net.fit(x_train, y_train, epochs=epoch_num, learning_rate=lr)\n",
        "\n",
        "print(f\"Total process time: {round(time.time() - t1,3)}\")\n",
        "\n",
        "output = improve_net.predict(x_test ,y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hhYkRRdCiKAu",
        "outputId": "52229af9-0ce8-4ab4-98b2-668a116dec0e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training epoch 1/20   error=0.393426\n",
            "Training epoch 2/20   error=0.127558\n",
            "Training epoch 3/20   error=0.093985\n",
            "Training epoch 4/20   error=0.073738\n",
            "Training epoch 5/20   error=0.063802\n",
            "Training epoch 6/20   error=0.059446\n",
            "Training epoch 7/20   error=0.052686\n",
            "Training epoch 8/20   error=0.050403\n",
            "Training epoch 9/20   error=0.044285\n",
            "Training epoch 10/20   error=0.042882\n",
            "Training epoch 11/20   error=0.041933\n",
            "Training epoch 12/20   error=0.039937\n",
            "Training epoch 13/20   error=0.042097\n",
            "Training epoch 14/20   error=0.031911\n",
            "Training epoch 15/20   error=0.034874\n",
            "Training epoch 16/20   error=0.034826\n",
            "Training epoch 17/20   error=0.037175\n",
            "Training epoch 18/20   error=0.032104\n",
            "Training epoch 19/20   error=0.028992\n",
            "Training epoch 20/20   error=0.029333\n",
            "Total process time: 1515.783\n",
            "\n",
            "Test set: Avg. loss: 0.1746, Accuracy: 9705/10000 (97%)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "שיפרתי באחוז :)\n"
      ],
      "metadata": {
        "id": "jB9yTNsnZk8S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!jupyter nbconvert --to html PS1_Part2_HelloNN_2024_ID_\\[207752643\\]\\ \\(2\\).ipynb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FsCq_xsNtwJw",
        "outputId": "3c3685dd-6969-4f58-8865-df89c830ff92"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[NbConvertApp] WARNING | pattern 'PS1_Part2_HelloNN_2024_ID_[207752643] (2).ipynb' matched no files\n",
            "This application is used to convert notebook files (*.ipynb)\n",
            "        to various other formats.\n",
            "\n",
            "        WARNING: THE COMMANDLINE INTERFACE MAY CHANGE IN FUTURE RELEASES.\n",
            "\n",
            "Options\n",
            "=======\n",
            "The options below are convenience aliases to configurable class-options,\n",
            "as listed in the \"Equivalent to\" description-line of the aliases.\n",
            "To see all configurable class-options for some <cmd>, use:\n",
            "    <cmd> --help-all\n",
            "\n",
            "--debug\n",
            "    set log level to logging.DEBUG (maximize logging output)\n",
            "    Equivalent to: [--Application.log_level=10]\n",
            "--show-config\n",
            "    Show the application's configuration (human-readable format)\n",
            "    Equivalent to: [--Application.show_config=True]\n",
            "--show-config-json\n",
            "    Show the application's configuration (json format)\n",
            "    Equivalent to: [--Application.show_config_json=True]\n",
            "--generate-config\n",
            "    generate default config file\n",
            "    Equivalent to: [--JupyterApp.generate_config=True]\n",
            "-y\n",
            "    Answer yes to any questions instead of prompting.\n",
            "    Equivalent to: [--JupyterApp.answer_yes=True]\n",
            "--execute\n",
            "    Execute the notebook prior to export.\n",
            "    Equivalent to: [--ExecutePreprocessor.enabled=True]\n",
            "--allow-errors\n",
            "    Continue notebook execution even if one of the cells throws an error and include the error message in the cell output (the default behaviour is to abort conversion). This flag is only relevant if '--execute' was specified, too.\n",
            "    Equivalent to: [--ExecutePreprocessor.allow_errors=True]\n",
            "--stdin\n",
            "    read a single notebook file from stdin. Write the resulting notebook with default basename 'notebook.*'\n",
            "    Equivalent to: [--NbConvertApp.from_stdin=True]\n",
            "--stdout\n",
            "    Write notebook output to stdout instead of files.\n",
            "    Equivalent to: [--NbConvertApp.writer_class=StdoutWriter]\n",
            "--inplace\n",
            "    Run nbconvert in place, overwriting the existing notebook (only\n",
            "            relevant when converting to notebook format)\n",
            "    Equivalent to: [--NbConvertApp.use_output_suffix=False --NbConvertApp.export_format=notebook --FilesWriter.build_directory=]\n",
            "--clear-output\n",
            "    Clear output of current file and save in place,\n",
            "            overwriting the existing notebook.\n",
            "    Equivalent to: [--NbConvertApp.use_output_suffix=False --NbConvertApp.export_format=notebook --FilesWriter.build_directory= --ClearOutputPreprocessor.enabled=True]\n",
            "--coalesce-streams\n",
            "    Coalesce consecutive stdout and stderr outputs into one stream (within each cell).\n",
            "    Equivalent to: [--NbConvertApp.use_output_suffix=False --NbConvertApp.export_format=notebook --FilesWriter.build_directory= --CoalesceStreamsPreprocessor.enabled=True]\n",
            "--no-prompt\n",
            "    Exclude input and output prompts from converted document.\n",
            "    Equivalent to: [--TemplateExporter.exclude_input_prompt=True --TemplateExporter.exclude_output_prompt=True]\n",
            "--no-input\n",
            "    Exclude input cells and output prompts from converted document.\n",
            "            This mode is ideal for generating code-free reports.\n",
            "    Equivalent to: [--TemplateExporter.exclude_output_prompt=True --TemplateExporter.exclude_input=True --TemplateExporter.exclude_input_prompt=True]\n",
            "--allow-chromium-download\n",
            "    Whether to allow downloading chromium if no suitable version is found on the system.\n",
            "    Equivalent to: [--WebPDFExporter.allow_chromium_download=True]\n",
            "--disable-chromium-sandbox\n",
            "    Disable chromium security sandbox when converting to PDF..\n",
            "    Equivalent to: [--WebPDFExporter.disable_sandbox=True]\n",
            "--show-input\n",
            "    Shows code input. This flag is only useful for dejavu users.\n",
            "    Equivalent to: [--TemplateExporter.exclude_input=False]\n",
            "--embed-images\n",
            "    Embed the images as base64 dataurls in the output. This flag is only useful for the HTML/WebPDF/Slides exports.\n",
            "    Equivalent to: [--HTMLExporter.embed_images=True]\n",
            "--sanitize-html\n",
            "    Whether the HTML in Markdown cells and cell outputs should be sanitized..\n",
            "    Equivalent to: [--HTMLExporter.sanitize_html=True]\n",
            "--log-level=<Enum>\n",
            "    Set the log level by value or name.\n",
            "    Choices: any of [0, 10, 20, 30, 40, 50, 'DEBUG', 'INFO', 'WARN', 'ERROR', 'CRITICAL']\n",
            "    Default: 30\n",
            "    Equivalent to: [--Application.log_level]\n",
            "--config=<Unicode>\n",
            "    Full path of a config file.\n",
            "    Default: ''\n",
            "    Equivalent to: [--JupyterApp.config_file]\n",
            "--to=<Unicode>\n",
            "    The export format to be used, either one of the built-in formats\n",
            "            ['asciidoc', 'custom', 'html', 'latex', 'markdown', 'notebook', 'pdf', 'python', 'qtpdf', 'qtpng', 'rst', 'script', 'slides', 'webpdf']\n",
            "            or a dotted object name that represents the import path for an\n",
            "            ``Exporter`` class\n",
            "    Default: ''\n",
            "    Equivalent to: [--NbConvertApp.export_format]\n",
            "--template=<Unicode>\n",
            "    Name of the template to use\n",
            "    Default: ''\n",
            "    Equivalent to: [--TemplateExporter.template_name]\n",
            "--template-file=<Unicode>\n",
            "    Name of the template file to use\n",
            "    Default: None\n",
            "    Equivalent to: [--TemplateExporter.template_file]\n",
            "--theme=<Unicode>\n",
            "    Template specific theme(e.g. the name of a JupyterLab CSS theme distributed\n",
            "    as prebuilt extension for the lab template)\n",
            "    Default: 'light'\n",
            "    Equivalent to: [--HTMLExporter.theme]\n",
            "--sanitize_html=<Bool>\n",
            "    Whether the HTML in Markdown cells and cell outputs should be sanitized.This\n",
            "    should be set to True by nbviewer or similar tools.\n",
            "    Default: False\n",
            "    Equivalent to: [--HTMLExporter.sanitize_html]\n",
            "--writer=<DottedObjectName>\n",
            "    Writer class used to write the\n",
            "                                        results of the conversion\n",
            "    Default: 'FilesWriter'\n",
            "    Equivalent to: [--NbConvertApp.writer_class]\n",
            "--post=<DottedOrNone>\n",
            "    PostProcessor class used to write the\n",
            "                                        results of the conversion\n",
            "    Default: ''\n",
            "    Equivalent to: [--NbConvertApp.postprocessor_class]\n",
            "--output=<Unicode>\n",
            "    Overwrite base name use for output files.\n",
            "                Supports pattern replacements '{notebook_name}'.\n",
            "    Default: '{notebook_name}'\n",
            "    Equivalent to: [--NbConvertApp.output_base]\n",
            "--output-dir=<Unicode>\n",
            "    Directory to write output(s) to. Defaults\n",
            "                                  to output to the directory of each notebook. To recover\n",
            "                                  previous default behaviour (outputting to the current\n",
            "                                  working directory) use . as the flag value.\n",
            "    Default: ''\n",
            "    Equivalent to: [--FilesWriter.build_directory]\n",
            "--reveal-prefix=<Unicode>\n",
            "    The URL prefix for reveal.js (version 3.x).\n",
            "            This defaults to the reveal CDN, but can be any url pointing to a copy\n",
            "            of reveal.js.\n",
            "            For speaker notes to work, this must be a relative path to a local\n",
            "            copy of reveal.js: e.g., \"reveal.js\".\n",
            "            If a relative path is given, it must be a subdirectory of the\n",
            "            current directory (from which the server is run).\n",
            "            See the usage documentation\n",
            "            (https://nbconvert.readthedocs.io/en/latest/usage.html#reveal-js-html-slideshow)\n",
            "            for more details.\n",
            "    Default: ''\n",
            "    Equivalent to: [--SlidesExporter.reveal_url_prefix]\n",
            "--nbformat=<Enum>\n",
            "    The nbformat version to write.\n",
            "            Use this to downgrade notebooks.\n",
            "    Choices: any of [1, 2, 3, 4]\n",
            "    Default: 4\n",
            "    Equivalent to: [--NotebookExporter.nbformat_version]\n",
            "\n",
            "Examples\n",
            "--------\n",
            "\n",
            "    The simplest way to use nbconvert is\n",
            "\n",
            "            > jupyter nbconvert mynotebook.ipynb --to html\n",
            "\n",
            "            Options include ['asciidoc', 'custom', 'html', 'latex', 'markdown', 'notebook', 'pdf', 'python', 'qtpdf', 'qtpng', 'rst', 'script', 'slides', 'webpdf'].\n",
            "\n",
            "            > jupyter nbconvert --to latex mynotebook.ipynb\n",
            "\n",
            "            Both HTML and LaTeX support multiple output templates. LaTeX includes\n",
            "            'base', 'article' and 'report'.  HTML includes 'basic', 'lab' and\n",
            "            'classic'. You can specify the flavor of the format used.\n",
            "\n",
            "            > jupyter nbconvert --to html --template lab mynotebook.ipynb\n",
            "\n",
            "            You can also pipe the output to stdout, rather than a file\n",
            "\n",
            "            > jupyter nbconvert mynotebook.ipynb --stdout\n",
            "\n",
            "            PDF is generated via latex\n",
            "\n",
            "            > jupyter nbconvert mynotebook.ipynb --to pdf\n",
            "\n",
            "            You can get (and serve) a Reveal.js-powered slideshow\n",
            "\n",
            "            > jupyter nbconvert myslides.ipynb --to slides --post serve\n",
            "\n",
            "            Multiple notebooks can be given at the command line in a couple of\n",
            "            different ways:\n",
            "\n",
            "            > jupyter nbconvert notebook*.ipynb\n",
            "            > jupyter nbconvert notebook1.ipynb notebook2.ipynb\n",
            "\n",
            "            or you can specify the notebooks list in a config file, containing::\n",
            "\n",
            "                c.NbConvertApp.notebooks = [\"my_notebook.ipynb\"]\n",
            "\n",
            "            > jupyter nbconvert --config mycfg.py\n",
            "\n",
            "To see all available configurables, use `--help-all`.\n",
            "\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}